# MathValidityPy
This repository contains Python code for analyzing and assessing the validity of mathematics diagnostic assessment for incoming undergraduate students.

## Math Assessment.ipynb
The Jupyter Notebook, **Math Assessment.ipynb**, contains a comprehensive analysis of math assessment data using Item Response Theory (IRT) models. The project utilizes the **py-irt** library and the Pyro Probabilistic Programming Framework to model student performance and item characteristics. The analysis proceeds through several stages, from data wrangling to model training, evaluation, and visualization.

### Key Features
**Data Preparation:** The notebook ingests raw student response data and item metadata, reshaping it into a long format suitable for IRT analysis. It also incorporates item domain and difficulty levels (**"EASY," "MEDIUM," "HARD"**) as covariates for more nuanced modeling.

**IRT Model Implementation:** It defines and trains four different IRT models:

**Rasch Model:** A baseline model estimating student ability and item difficulty.

**2PL Model**: An extension of the Rasch model that also accounts for item discrimination.
**2PL Model: An extension of the Rasch model that also accounts for item discrimination.

**2PL Model**: An extension of the Rasch model that also accounts for item discrimination.
**Rasch + Covariates**: Models that include item domain and difficulty levels as covariates to explore their effects on item difficulty.

**PL + Covariates**: The most complex models, incorporating item discrimination, base difficulty, and the effects of item domain and difficulty levels.

**Model Evaluation:** The models are trained using Stochastic Variational Inference (SVI), and their fit is assessed through Posterior Predictive Checks (PPC). The PPC plots compare the distribution of observed total scores with replicated scores generated by each model, providing a visual assessment of model adequacy.

**Parameter Analysis:** After training, the notebook extracts and displays the estimated parameters for each item, including discrimination ($\alpha$), base difficulty ($\beta_{base}$), domain shifts ($\delta$), and difficulty shifts ($\gamma$). This allows for a detailed understanding of how different factors contribute to an item's overall difficulty.

**Visualization:** The project includes functionality to generate **Item Characteristic Curves (ICCs)** for each item, providing a visual representation of the relationship between student ability and the probability of a correct response. It also includes empirical ICCs, which plot the model's predicted curve against the actual student data..

This project serves as a practical example of applying advanced Bayesian IRT models to educational data, demonstrating how to incorporate external covariates to build more robust and interpretable psychometric models.

## Empirical_Plots_math and ICC_Plots_math Folders
The Empirical Plots and ICC Plots visualize the analysis using Item Response Theory (IRT). 
The empirical plots contain scatter plots showing the actual student responses (0 for incorrect, 1 for correct) against their estimated abilities. 

The ICC_Plots folder houses Item Characteristic Curves (ICCs) for each item. These plots depict the theoretical relationship between a student's ability ($\theta$) and their probability of answering an item correctly, based on the estimated difficulty (b) and discrimination (a) parameters from the IRT model. 






